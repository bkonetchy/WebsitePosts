---
title: "GTFS Time Table Viewer"
author: "Brant Konetchy"
date: "2023-09-06"
categories: ["R"]
tags: ["R", "leaflet", "GTFS"]
output: html_document
---

```{r setup, include=FALSE}

library(sf)
library(dplyr)
library(leaflet)
library(data.table)

# load gtfs file
gtfs <- gtfstools::read_gtfs(path = "../../Datasets/post2_gtfs.zip")

NUTS <- st_read(dsn = "../../Datasets/NUTS_RG_01M_2021_4326.shp", 
                layer = "NUTS_RG_01M_2021_4326") %>% 
  filter(LEVL_CODE == 3) %>% 
  filter(CNTR_CODE == "DE") %>% 
  filter(NUTS_NAME == "Berlin") %>%
  st_transform(crs = 3035) %>% # convert to projected meter system covering germany
  st_buffer(dist = 5000) %>% # extract extra area round berlin by 5 km
  st_transform(crs = "WGS84") # convert back to WGS84

leaflet() %>%
  addTiles() %>%
  addPolygons(data = NUTS)
```

# GTFS Time Table Viewer

This post will go through how to process a GTFS file in order to view the results as a standard time table. We want a result that is similar to standard time tables like those found here: . By processing GTFS results in this manor produces a table that is both easy to read and easy to check for any issues with the GTFS feed. This can be very helpful when trying to debug or check GTFS for accuracy and let non-technical users easily access the time tables within a GTFS file.

## Step 1: Read in GTFS feed

Any GTFS feed can be used, but in this example I will be using the Germany wide GTFS feed produced by DELFI ([GTFS Germany](https://www.opendata-oepnv.de/fileadmin/datasets/delfi/20230904_fahrplaene_gesamtdeutschland_gtfs.zip)) and extracting the data that intersects Berlin. For the Berlin boarder shape I downloaded the [NUTS dataset](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts). Warning for a large GTFS file it can take up to a few minutes to read in the GTFS feed.

```{r message=FALSE, warning=FALSE, eval=FALSE}
library(sf)
library(dplyr)
library(leaflet)
library(data.table)

# read in gtfs feed
gtfs <- tidytransit::read_gtfs(path = "../../Datasets/20230904_fahrplaene_gesamtdeutschland_gtfs.zip")

# read in NUTS polylines and filter to berlin
NUTS <- st_read(dsn = "../../Datasets/NUTS_RG_01M_2021_4326.shp", 
                layer = "NUTS_RG_01M_2021_4326") %>% 
  filter(LEVL_CODE == 3) %>% 
  filter(CNTR_CODE == "DE") %>% 
  filter(NUTS_NAME == "Berlin") %>%
  st_transform(crs = 3035) %>% # convert to projected meter system covering germany
  st_buffer(dist = 5000) %>% # extract extra area round berlin by 5 km
  st_transform(crs = "WGS84") # convert back to WGS84


```

```{r, out.width="100%"}
# take a look at the area we want to extract
leaflet() %>%
  addTiles() %>%
  addPolygons(data = NUTS)
```

## Step 2: Filter for Exact Date and Berlin Area

When working with GTFS, I found it generally more useful to filter for an exact date rather then just a generic day during the week. The main reason for this is that a route can have minimal or drastic changes depending on planned construction projects. By selecting an exact date we can evaluate the route to see if it is going on the attended "normal" path or if it is being deviated for some reason. This can be especially important if the GTFS feed is being using with routing software like that in the R5R package. The R5R package requires a single day to run the routing analysis, so we want to ensure that that day chose aligns with what we expect travelers to use or we want them to use. This can lead to some odd coding decisions in this case, in which I will switch between two different GTFS processing packages (gtfsools) and (tidytransit). In the first step I used tidytransit to read in the dataset so that I could use the filter by exact day function. This causes the initial load time to be longer as tidytransit generally takes longer to read in GTFS files as gtfstools. However, after we finish the filter by day, we want to convert to the gtftools data type for the rest of the analysis. Luckily, both packages come with converters making it easy to switch back and forth between the two. Generally I would also advise to work with smaller feeds due to the time cost of filtering larger feeds, but when not possible its best to filter out as much data as possible before performing the spatial filter as I do here. In this example the size of the dataset decresses drastically from each filter with a starting size of 2.9 gb and ending at about 104 mb.

```{r eval=FALSE}
# filter by date
gtfs <- tidytransit::filter_feed_by_date(gtfs_obj = gtfs, 
                                         extract_date = "2023-10-17")

# convert back to gtfstools data type
gtfs <- gtfstools::as_dt_gtfs(gtfs)

# filter area to just Berlin
gtfs <- gtfstools::filter_by_sf(gtfs = gtfs, 
                                geom = st_geometry(NUTS)) # here I used st_geometry to reduce the sf datset to a sfc, or just the geometry
```

## Step 3: Extract Route for Processing

Now that we have a GTFS feed filtered for an exact date lets select a single route to use. In this example I will use Bus line 222 that runs between "" and "". In order to get the correct data to create the time table we will have to perform some filters to extract the datasets we need. I will do this step by step.

```{r}

# find the route id for bus line 222
route_id <- gtfs$routes[route_short_name %like% 222]$route_id

# use gtfstools filter by route id function
route_222 <- gtfstools::filter_by_route_id(gtfs = gtfs, route_id = route_id)

# Extract trip ids per direction
trip_ids_0 <- route_222$trips[direction_id == 0]$trip_id
trip_ids_1 <- route_222$trips[direction_id == 1]$trip_id

# extract all stop ids associated with the route in each direction
stop_ids_0 <- route_222$stop_times[trip_id %in% trip_ids_0]$stop_id
stop_ids_1 <- route_222$stop_times[trip_id %in% trip_ids_1]$stop_id

# find the stop times associated with each trip in each direction
stop_times_0 <- route_222$stop_times[trip_id %in% trip_ids_0]
stop_times_1 <- route_222$stop_times[trip_id %in% trip_ids_1]

# stop parent ids, if they are listed in the gtfs we can use, if not we can create ourselves useing the german station id system
stop_parent_ids <- gtfstools::get_parent_station(gtfs = route_222)
stop_parent_ids
# parent stations do not extis so we will create ourselves
stop_parent_ids_0 <- data.table("stop_id" = stop_ids_0,
                                "parent_id" = lapply(X = stop_ids_0, 
                                                     FUN = function(x) {
                                                       stringr::str_split_1(string = x, 
                                                                            pattern = "::")}) %>% 
                                  do.call(rbind, .) %>% 
                                  as.data.frame() %>% 
                                  select(1)) %>%
  distinct()

stop_parent_ids_1 <- data.table("stop_id" = stop_ids_1,
                                "parent_id" = lapply(X = stop_ids_1, 
                                                     FUN = function(x) {
                                                       stringr::str_split_1(string = x, 
                                                                            pattern = "::")}) %>% 
                                  do.call(rbind, .) %>% 
                                  as.data.frame() %>% 
                                  select(1)) %>%
  distinct()

```

## Step 4: Produce the Final Time Table

Now that we have the time table data components extracted, lets put them together to produce our final time table.

### Step 1: Order Stops by Earliest Arrival Time

This section of code finds the earliest arrival time for every stop and then orders it from the earliest time. We then add a new column with the order of the earliest to latest stop arrival.

```{r}
stop_order_0 <- stop_times_0[,.("arrival_time" = min(arrival_time)), stop_id]
# merge in parent ids and reorder only using parent ids
stop_order_0 <- merge(stop_order_0, stop_parent_ids_0, by = "stop_id")
stop_order_0 <- stop_order_0 %>% 
  group_by(parent_id.V1) %>% 
  filter(arrival_time == min(arrival_time)) %>% 
  arrange(arrival_time) %>%
  as.data.table()
stop_order_0$stop_order <- 1:nrow(stop_order_0)

# table for direction 1
stop_order_1 <- stop_times_1[,.("arrival_time" = min(arrival_time)), stop_id][order(arrival_time)]
# merge in parent ids and reorder only using parent ids
stop_order_1 <- merge(stop_order_1, stop_parent_ids_1, by = "stop_id")
stop_order_1 <- stop_order_1 %>% 
  group_by(parent_id.V1) %>% 
  filter(arrival_time == min(arrival_time)) %>% 
  arrange(arrival_time) %>%
  as.data.table()
stop_order_1$stop_order <- 1:nrow(stop_order_1)
```

### Step 2: Order Trips by Earliest Arrival Time

This section of code is very similar to the previous except this time we are ordering the trips by earliest arrival time.

```{r}
trip_order_0 <- stop_times_0[,.("arrival_time" = min(arrival_time)), trip_id][order(arrival_time)]
trip_order_0$trip_order <- 1:nrow(trip_order_0)
trip_order_1 <- stop_times_1[,.("arrival_time" = min(arrival_time)), trip_id][order(arrival_time)]
trip_order_1$trip_order <- 1:nrow(trip_order_1)
```

### Step 3: Merge in Stop Times

This step merges back the stop times with the stop and trip order columns. We will also merge in the stop table that contains the name of each stop to use in the final table.

```{r}
stop_times_0 <- merge(stop_times_0, 
                      y = stop_order_0[,.(stop_id, stop_order)], 
                      by = "stop_id") %>%
  merge(y = trip_order_0[,.(trip_id, trip_order)], by = "trip_id") %>%
  merge(y = route_222$stops[,.(stop_id, stop_name)], by = "stop_id")

stop_times_1 <- merge(stop_times_1, 
                      y = stop_order_1[,.(stop_id, stop_order)], 
                      by = "stop_id") %>%
  merge(y = trip_order_1[,.(trip_id, trip_order)], by = "trip_id") %>%
  merge(y = route_222$stops[,.(stop_id, stop_name)], by = "stop_id")

```

### Step 4: Convert the Stop Times into a Wide Format

```{r, out.width="100%"}
# create table for direction 0 by casting to wide format
stop_times_0_wide <- data.table::dcast.data.table(data = stop_times_0, formula = stop_order+stop_name+stop_id ~ trip_order, value.var = "arrival_time")
# clean colnames
colnames(stop_times_0_wide) <- c("Stop Order", "Stop Names","Stop_ID", 
                            paste0("Trip_", 1:(length(stop_times_0_wide)-3)))

# create table for direction 0 by casting to wide format
stop_times_1_wide <- data.table::dcast.data.table(data = stop_times_1, formula = stop_order+stop_name+stop_id ~ trip_order, value.var = "arrival_time")
# clean colnames
colnames(stop_times_1_wide) <- c("Stop Order", "Stop Names","Stop_ID", 
                            paste0("Trip_", 1:(length(stop_times_1_wide)-3)))


```

### Step 5: Display Time Table

```{r}
library(reactable)
library(reactablefmtr)

# view the results in a nice table
reactable(data = stop_times_0_wide, pagination = F, height = 800, 
          theme = reactableTheme(color = "black")) %>%
  add_title("Bus Line 222 Time Table") %>%
  add_subtitle("Direction 0")

# view the results in a nice table
reactable(data = stop_times_1_wide, pagination = F, height = 800, 
          theme = reactableTheme(color = "black")) %>%
  add_title("Bus Line 222 Time Table") %>%
  add_subtitle("Direction 1")

```

## Step 5: Check Results

Now that we have created the time table lets perform a series of manual and automated checks to see if the results we have produced are correct. For the manual checks, we are going to find the actual time table posted by the BVG (Berlin's public transportation organization) and check if the stop order and a few of the trips appear correct. Checking all trips would be to time consuming, so we will also performing a few logic test to ensure the data in the table is as we expect. First lets take a look at the Bus route 222 time table ().

Now that we have performed some manual checks lets do some automated checks. We will perform two basic checks that will ensure that the table is in the correct order. The first will check each column and makes sure that the value in the next row is always greater than the previous value, as we should always be increasing in time when going to the next stop. The second check will look at each row and perform the same test but looking at each trip (column) should start at a later time as the previous trip.

```{r}

# create a function to check all values in a column
column_check_fun <- function(time_values) {
  
  # remove any nas from vector (effectivly removes stops not being stopped at)
  time_values <- time_values[!is.na(time_values)]
  
  # lag vector
  lag_vect <- time_values[1:(length(time_values)-1)]
  # lead vector
  lead_vector <- time_values[2:length(time_values)]
  # check if the lead value is greater than the lag value
  test <- lead_vector > lag_vect
  return(all(test))
}

# run checks on columns
col_checks <- apply(X = stop_times_0_wide[,c(-1,-2,-3)], MARGIN = 2, FUN = column_check_fun)

# see if all values are true or not
all(col_checks)

# we have false values so lets extrac the trips with an error and perform a more indepth analysis 
trip_errors <- which(col_checks == F)

# go through each trip and return the lead stop(s) that is causing the issue
trip_errors <- lapply(trip_errors, FUN = function(x) {
  
  # subset for trip, order by stop order, and find diff in stop sequence
  temp <- stop_times_0[trip_order == x][order(stop_order), diff(x = stop_sequence)]
  
  # find vlue that does not equal 1 (diff in the sequence should always be 1)
  hold <- (which(temp != 1):length(temp)) + 1 # add 1 to get the actual stop with the issue, we want all values after this as well
  
  # return the stop information with the issue
  stop_times_0[trip_order == x][order(stop_order)][hold]
  
}) %>%
  do.call(rbind, .)

# find which stops are causing the issue
stops_to_change <- trip_errors[,unique(stop_id)]

# assume that placing these stops at some higger stop order should produce the correct results, will loop through each attempt until all times are correct or fail after going through the complete stop order
lapply(stops_to_change, FUN = function(stops_change) {
  
  # create new stop order by starting by placing the change stop in the first postion and working down until the times are in the correct order
  i <- 1
  while (i < nrow(stop_times_0_wide)) {
    # find current stop order
    cur_stop_order <- stop_times_0_wide[Stop_ID == stops_change]$`Stop Order`
    # make desired stop order one value higher
     stop_times_0_wide[`Stop Order` == cur_stop_order - 1]$`Stop Order` <- cur_stop_order
    # change the stop order postion
    stop_times_0_wide[Stop_ID == stops_change]$`Stop Order` <- cur_stop_order - 1
    # save current version of stop times 0 wide to global env
    stop_times_0_wide <<- stop_times_0_wide[order(`Stop Order`)]
    # rerun col checks and see if all values are true
    col_checks <- apply(X = stop_times_0_wide[,c(-1,-2,-3)], MARGIN = 2, 
                        FUN = column_check_fun)
    # increase postion
    i <- i + 1
    # break if all values are true, if not repeat with the next position
    if (all(col_checks)) {
      print('found')
      break
    }
    
  }
  
  
})

# remake stop order starting from 1
stop_times_0_wide$`Stop Order` <- 1:nrow(stop_times_0_wide)


```

Recheck and make sure that fixed the issue or not.

```{r}
# run checks on columns
col_checks <- apply(X = stop_times_0_wide[,c(-1,-2,-3)], MARGIN = 2, FUN = column_check_fun)

# see if all values are true or not
all(col_checks)


```

That did fix our column issues. So now lets check by row.



```

## Step 6: Show Line Route and Stops (Optional)

In this section we will display a single trip from bus line 222 along with all the stops and stop times. To do this we will simply extract geospatial information using the select trip ID and then use Leaflet to display the results.

## Future Plans

While this post only showed the results for a single line, we can convert all this logic into a shiny app that can load in any GTFS feed, filter the feed by given time and date, and then select any of the routes and produce the time table and map for each one. Putting this code into a shiny app will allow users to quickly check the time tables of each route and ensure that they are running as expected. Once the shiny app is complete I will create a new post going over the details of the app and sharing the app itself.
